Systematic examination of model errors through confusion matrices, training curves, and qualitative prediction samples revealed distinct failure patterns for each architecture. The ResNet18 confusion matrix showed 108 false positives (68\% of non-fire images misclassified as fire) versus only 50 false negatives, indicating the model learned to err on the side of caution by over-predicting fires. Visual inspection of misclassifications revealed that ResNet18 consistently failed on ambiguous atmospheric conditions—sunsets with orange/red coloring (predicted as fire with 0.482 confidence), fog banks resembling smoke, and cloud formations with similar visual characteristics to fire plumes. The model's low confidence scores (0.3-0.6 range) on correct predictions further suggested fundamental uncertainty in its learned features. In contrast, MobileNetV2 demonstrated robust performance across diverse conditions with high-confidence predictions on both fire (0.817-0.999) and non-fire (0.309-0.358 for correct rejections) scenarios, though it occasionally struggled with heavily obscured or distant fires. EfficientNet-B0's error patterns were less systematic and more chaotic, with the validation accuracy oscillating wildly between 40-63\% and misclassifying obvious cases, indicating the model failed to learn generalizable features and instead memorized training data.
The models exhibited clear performance differences aligned with their architectural characteristics and training stability. MobileNetV2 excelled at distinguishing subtle visual differences between fire-related smoke (gray, billowing, rising patterns) and benign atmospheric effects (uniform fog, wispy clouds), likely due to its stable training convergence where validation and training losses tracked closely together. ResNet18 showed strength in detecting obvious fires with visible flames or dense smoke columns (80\% recall) but consistently triggered false alarms on any reddish or orange-tinted imagery, suggesting it over-relied on color features rather than texture and spatial patterns. The threshold analysis revealed this wasn't simply a calibration issue—even at optimal thresholds (0.6-0.7), ResNet18 could only achieve approximately 70\% recall with 50-60\% precision, indicating fundamental limitations in its learned feature representations. EfficientNet-B0's performance degradation after epoch 6, where validation loss spiked from 0.66 to 0.92 while training loss continued decreasing, demonstrated classic overfitting where the model memorized training examples rather than learning transferable patterns for fire detection.
Pattern analysis across error types revealed several systematic issues requiring targeted interventions. First, all models struggled with "fire-like" non-fire scenarios: sunsets with warm lighting (orange/red sky), dust clouds with similar texture to smoke, and fog in mountainous terrain resembling distant fire smoke. ResNet18's 108 false positives concentrated heavily in these categories, while even MobileNetV2 occasionally misclassified heavily backlit smoke-like formations. Second, the 61:39 class imbalance toward fire instances contributed to ResNet18's bias toward over-prediction, as the model optimized for overall accuracy rather than balanced class performance. Third, inference time variability (ResNet18: 245-2880ms) suggested computational inefficiencies that would impact real-time deployment. To address these issues, future work should: (1) augment the training dataset with challenging non-fire examples specifically targeting failure modes (sunset images, fog, dust, clouds with fire-like colors/textures), (2) implement class-balanced loss functions or focal loss to penalize confident misclassifications and address the dataset imbalance, (3) employ stronger data augmentation including color jittering, atmospheric effects simulation, and lighting condition variations to reduce color bias, (4) implement early stopping around epoch 6-8 for ResNet18 based on validation performance plateaus, (5) explore ensemble approaches combining ResNet18's high recall (80\%) with MobileNetV2's high precision (~90\%) to achieve optimal safety-efficiency balance, and (6) incorporate temporal context from image sequences rather than single frames to leverage fire progression patterns that distinguish actual fires from static atmospheric conditions.