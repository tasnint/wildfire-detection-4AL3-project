\subsection{Systematic Error Examination}
Systematic examination of model errors through confusion matrices, training curves, and qualitative prediction samples revealed distinct failure patterns for each architecture. The ResNet18 confusion matrix (Figure~\ref{fig:resnet_confusion}) showed 108 false positives (67.92\% of non-fire images misclassified as fire) versus only 50 false negatives (19.92\% miss rate), indicating the model learned to err excessively on the side of caution by over-predicting fires. This asymmetric error pattern suggests the model's decision boundary was significantly biased toward the fire class, likely influenced by the 61.3:38.7 class imbalance in the training data combined with insufficient regularization during training from scratch without pre-trained weights. Visual inspection of misclassifications (Figure~\ref{fig:resnet_predictions}) revealed that ResNet18 consistently failed on ambiguous atmospheric conditions—sunsets with orange/red coloring (predicted as fire with 0.482 confidence), fog banks resembling smoke, cloud formations with similar visual characteristics to fire plumes, and any landscape with warm-toned lighting. The model's low confidence scores (0.309-0.654 range) even on correct predictions further suggested fundamental uncertainty in its learned features, with the ROC curve (Figure~\ref{fig:resnet_roc}) showing an AUC of only 0.629—barely better than random guessing. The precision-recall curve (AP=0.737) demonstrates that precision degrades rapidly as recall increases, confirming that no threshold adjustment can overcome the model's fundamental discriminative limitations.
\begin{figure}[h]
\centering
\includegraphics[width=0.3\textwidth]{graphs/resnet18_confusion_matrix.png}
\caption{ResNet18 confusion matrix revealing severe imbalance in error patterns: 108 false positives (67.92\% of non-fire images misclassified) compared to 50 false negatives (19.92\% miss rate). The asymmetric distribution indicates systematic over-prediction of fires, resulting in 80.1\% recall but only 50.5\% precision.}
\label{fig:resnet_confusion}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{graphs/resnet18_roc_pr_threshold_analysis.png}
\caption{ResNet18 performance analysis across multiple dimensions: (left) ROC curve with AUC=0.629 indicating weak discriminative ability barely above random chance; (center) Precision-Recall curve with AP=0.737 showing rapid precision degradation as recall increases; (right) Threshold analysis demonstrating the trade-off between false positive rate (FPR) and true positive rate (TPR), with optimal threshold around 0.3-0.4 achieving 94.4\% recall but only 61.9\% precision.}
\label{fig:resnet_roc}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{graphs/resnet18_sample_predictions.png}
\caption{ResNet18 sample predictions showing common failure modes: (top row, second image) sunset misclassified as fire with 0.482 confidence due to orange/red atmospheric coloring; multiple low-confidence correct predictions (0.309-0.358 range) indicating model uncertainty; and systematic failures on fog, clouds, and atmospheric effects that visually resemble smoke.}
\label{fig:resnet_predictions}
\end{figure}
In contrast, EfficientNet-B0 demonstrated catastrophic failure with a fundamentally different error pattern (Figure~\ref{fig:efficientnet_confusion}). The confusion matrix shows 72 true negatives, 87 false positives, 34 false negatives, and 217 true positives, yielding 70.5\% accuracy, 71.4\% precision, and 86.5\% recall. However, these seemingly reasonable metrics mask the severe training instability visible in Figure~\ref{fig:efficientnet_curves}. The validation accuracy oscillated wildly between 40-63\% across epochs, with a dramatic collapse after epoch 6 where validation loss spiked from 0.66 to 0.92 while training loss continued decreasing smoothly from 0.68 to 0.59. This classic overfitting pattern indicates the model memorized training data rather than learning generalizable features. Sample predictions (Figure~\ref{fig:efficientnet_predictions}) show inconsistent and chaotic classifications—the model assigns vastly different confidence scores to visually similar images and misclassifies obvious cases that simpler models handle correctly. The high ROC-AUC of 0.779 and average precision of 0.849 on the test set are misleading given the validation instability, suggesting these metrics benefited from fortunate test set characteristics rather than genuine model capability.
\begin{figure}[h]
\centering
\includegraphics[width=0.3  \textwidth]{graphs/effnet_confusion_matrix.png}
\caption{EfficientNet-B0 confusion matrix showing 87 false positives (54.7\%) and 34 false negatives (13.5\%). While these error rates appear more balanced than ResNet18, they must be interpreted in context of the severe training instability shown in Figure~\ref{fig:efficientnet_curves}.}
\label{fig:efficientnet_confusion}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[width=0.3\textwidth]{graphs/efficientnet_predictions.png}
\caption{EfficientNet-B0 sample predictions demonstrating inconsistent classification behavior. Note the highly variable inference times (180-2880ms) and chaotic confidence patterns, with the model showing uncertainty even on clear fire images and producing erratic predictions on atmospheric conditions.}
\label{fig:efficientnet_predictions}
\end{figure}
MobileNetV2 demonstrated robust performance across diverse conditions with high-confidence predictions on both fire (0.82-0.999 range) and non-fire (correct rejections with appropriate confidence) scenarios (Figure~\ref{fig:mobilenet_predictions}), though it occasionally struggled with heavily obscured or distant fires. The model's stable training curves (Figure~\ref{fig:mobilenet_curves}) show training and validation metrics tracking closely together—training accuracy reaches 100\% while validation accuracy stabilizes at 88\%, with both losses converging smoothly. This minimal overfitting gap indicates the model learned genuine discriminative features rather than memorizing training patterns.
\begin{figure}[h]
\centering
\includegraphics[width=0.3\textwidth]{graphs/mobilenet_wildfire_sample_predictions.png}
\caption{MobileNetV2 sample predictions demonstrating consistently high-confidence correct classifications (0.82-0.999 for fires, appropriate confidence for non-fires) and robust performance across diverse conditions including smoke, atmospheric effects, and challenging lighting. The model shows strong generalization with fast, consistent inference times (180-320ms).}
\label{fig:mobilenet_predictions}
\end{figure}
\subsection{Model Performance Comparison}
The models exhibited clear performance differences aligned with their architectural characteristics and training stability. MobileNetV2 excelled at distinguishing subtle visual differences between fire-related smoke (gray, billowing, rising patterns with specific textures) and benign atmospheric effects (uniform fog, wispy clouds, sunset coloring), likely due to its stable training convergence where validation and training losses tracked closely together throughout all epochs (Figure~\ref{fig:mobilenet_curves}). The architecture's depthwise separable convolutions efficiently learned multi-scale features while maintaining computational efficiency, enabling both high accuracy and fast inference times. ResNet18 showed strength in detecting obvious fires with visible flames or dense smoke columns (80.1\% recall, 201 out of 251 fires detected) but consistently triggered false alarms on any reddish or orange-tinted imagery, suggesting it over-relied on color features rather than texture and spatial patterns that better distinguish actual fires from atmospheric phenomena. The threshold analysis (Figure~\ref{fig:resnet_roc}, right panel) revealed this wasn't simply a calibration issue—even at the optimal threshold of 0.30 that maximizes recall (94.4\%) while maintaining acceptable precision (61.9\%), the model still produces substantial false alarm rates. At the default threshold of 0.50, the model achieves 80.1% recall but only 50.5% precision, producing two false alarms for every correctly detected fire. This fundamental limitation stems from the model's weak discriminative power (ROC-AUC = 0.629) rather than suboptimal threshold selection.
EfficientNet-B0's performance degradation after epoch 6, where validation loss spiked from 0.66 to 0.92 while training loss continued decreasing from 0.68 to 0.59 (Figure~\ref{fig:efficientnet_curves}), demonstrated classic overfitting where the model memorized training examples rather than learning transferable patterns for fire detection. The compound scaling approach of EfficientNet-B0, while effective with proper regularization and pre-trained weights, proved too complex for training from scratch on this relatively small dataset. The model's 239 layers (with 143 frozen and 96 trainable in the attempted fine-tuning configuration) required careful regularization that was clearly insufficient—the multiple dropout layers (0.5, 0.4, 0.3) and the frozen base layers could not prevent the validation collapse. The erratic validation metrics visible in Figure~\ref{fig:resnet_training_curves} for ResNet18 (precision oscillating between 0\% and 100\%, recall between 0\% and 75\%) similarly indicate training instability, though less severe than EfficientNet-B0's catastrophic failure.
\subsection{Error Patterns and Future Improvements}
To address these issues, future work should: (1) augment the training dataset with challenging non-fire examples targeting failure modes (sunset images, fog, dust clouds, fire-like atmospheric phenomena), increasing dataset size to at least 5,000-10,000 images per class; (2) implement class-balanced or focal loss functions to address dataset imbalance; (3) employ stronger data augmentation including color jittering, atmospheric effects simulation, and lighting variations to reduce color bias; (4) implement early stopping around epoch 6-8 for ResNet18 based on validation performance plateaus; (5) explore ensemble approaches combining ResNet18's high recall with MobileNetV2's high precision through weighted voting.