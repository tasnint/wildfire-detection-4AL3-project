\subsection{Dataset Split Strategy}
The evaluation strategy employed a standard 70/15/15 train-validation-test split on a dataset containing 2,700 images with a 61.3:38.7 non-fire-to-fire class distribution. The training set (1,887 images: 1,157 non-fire, 730 fire) was used for model learning and weight optimization, the validation set (403 images: 246 non-fire, 157 fire) for hyperparameter tuning and early stopping decisions during training, and the test set (410 images: 251 non-fire, 159 fire) for final unbiased performance assessment. This represents a notable class imbalance with fire instances comprising only 38.7\% of the dataset, which significantly influenced model behavior as discussed in the error analysis section. Cross-validation was not implemented due to computational constraints of training deep neural networks and the desire to ensure all three architectures (MobileNetV2, ResNet18, and EfficientNet-B0) were evaluated on identical test samples for fair comparison. While the single split approach enabled consistent comparisons and the test set size was sufficient for reliable estimates, future work should incorporate k-fold cross-validation to better quantify performance variance and ensure findings generalize across different data partitions.
\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{graphs/mobilenet_training_curves.png}
\caption{MobileNetV2 training and validation curves demonstrating excellent convergence with minimal overfitting. The model achieves near-perfect training accuracy (100\%) with stable 88\% validation accuracy throughout training, indicating strong generalization capability.}
\label{fig:mobilenet_curves}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[width=0.3\textwidth]{graphs/effnet_confusion_matrix.png}
\caption{EfficientNet-B0 training and validation curves showing catastrophic failure. Note the validation accuracy collapse from 63\% to 40\% after epoch 6, accompanied by validation loss explosion from 0.66 to 0.92, indicating severe overfitting and training instability.}
\label{fig:efficientnet_curves}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[width=0.4\textwidth]{graphs/resnet18_training_curves.png}
\caption{ResNet18 comprehensive training curves showing accuracy, loss, precision, recall, and AUC over epochs. Validation metrics plateau around 40-60\% with high instability and erratic precision-recall patterns, while training accuracy reaches 83\%, indicating moderate overfitting and fundamental learning limitations.}
\label{fig:resnet_training_curves}
\end{figure}
\subsection{Evaluation Metrics}
The evaluation utilized a comprehensive suite of metrics that evolved significantly from the progress report stage, where accuracy and training loss were likely the primary focus. Early analysis revealed these metrics were inadequate—accuracy alone masked critical issues like ResNet18's 67.92\% false positive rate (108 of 159 non-fire images misclassified) and EfficientNet-B0's catastrophic validation collapse. The refined evaluation framework incorporated precision to quantify false alarm rates (MobileNetV2: $\sim$90\%, ResNet18: 50.5\%, EfficientNet-B0: 67.9\%), recall to measure fire detection capability (MobileNetV2: $\sim$85\%, ResNet18: 80.1\%, EfficientNet-B0: 86.5\%), F1-score for balanced assessment (ResNet18: 61.5\% overall), ROC-AUC for threshold-independent performance evaluation (ResNet18: 0.629, EfficientNet-B0: 0.779), confusion matrices for error pattern analysis (Figures~\ref{fig:resnet_confusion} and \ref{fig:efficientnet_confusion}), precision-recall curves (ResNet18 AP=0.737, Figure~\ref{fig:resnet_roc}), threshold sensitivity analysis (Figure~\ref{fig:resnet_threshold}), training/validation loss convergence tracking (Figures~\ref{fig:mobilenet_curves}, \ref{fig:efficientnet_curves}, \ref{fig:resnet_training_curves}), and inference time measurements (MobileNetV2: 180-320ms, ResNet18: variable, EfficientNet-B0: 180-2880ms). These expanded metrics proved essential for understanding real-world deployment viability in safety-critical wildfire detection applications.
\subsection{Metric Adequacy Assessment}
The metric adequacy assessment confirmed that the progress report metrics were insufficient for this application domain. Accuracy failed to distinguish between different error types—critically important in a context where missing an actual fire has catastrophic consequences (loss of life, property destruction) while false positives merely waste emergency response resources. The comprehensive evaluation revealed stark differences in model behavior that accuracy alone obscured. ResNet18 achieved 61.5\% test accuracy but with an alarming 67.92\% false positive rate, meaning the model predicted fire for 108 out of 159 non-fire images. While its 80.1\% recall (201 fires correctly detected out of 251) appears acceptable from a pure safety perspective, the precision of only 50.5\% renders it impractical—producing two false alarms for every actual fire detected. EfficientNet-B0's validation loss explosion (Figure~\ref{fig:efficientnet_curves}) from 0.66 to 0.92 after epoch 6, coupled with validation accuracy collapse from 63\% to 40\%, indicated severe overfitting requiring complete retraining with adjusted hyperparameters, stronger regularization, and possibly pre-trained ImageNet weights. The model's final test results show 70.5\% accuracy with 71.4\% precision and 86.5\% recall, but the training instability makes these results unreliable. MobileNetV2 emerged as the optimal solution with 88\% validation accuracy (Figure~\ref{fig:mobilenet_curves}), balanced precision-recall performance, minimal overfitting gap between training (100\%) and validation (88\%), high-confidence predictions (0.82-0.999 range for fires, see Figure~\ref{fig:mobilenet_predictions}), and deployment-ready stability across all evaluation metrics. The model's training and validation losses converged together smoothly, indicating genuine learning of generalizable features rather than memorization of training data.