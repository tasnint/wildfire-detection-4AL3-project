\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[numbers]{natbib}
\usepackage[]{acl}
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{graphicx}


\title{Group 73 Progress Report:\\Wildfire Detection Classification Plan}


\author{Andy Huynh, Berk Yilmaz, Tanisha Tasnin \\
  \textbf{huynha3@mcmaster.ca}, \textbf{yilmag1@mcmaster.ca},  \textbf{tasnint@mcmaster.ca}}

\begin{document}
\maketitle

\section{Introduction}

% Here, write a brief introduction to the problem you are solving. This can be adapted from your problem description and motivation from the original proposal. This should be around 0.25-0.5 pages.
\input{1_Introduction.tex}

% Here, talk about the related work you encountered for your approach. Cite at least 5 references. Refer to item 2. No one has done exactly your task? Write about the most similar thing you can find. This should be around 0.25-0.5 pages.
\section{Related Work}
\input{2_Related_Work.tex}

% You should write about your dataset here, following the guidelines regarding item 1. This section may be 0.5-1 pages. Depending on your specific dataset, you may want to include subsections for the preprocessing, annotation, etc.
\section{Dataset}
\input{3_Dataset.tex}

% Describe any features you used for your model, or how your data was input to your model. Are you doing feature engineering or feature selection? Are you learning embeddings? Is it all part of one neural network? Refer to item 2. This may range from 0.25 pages to 0.5 pages.
\section{Features}
\input{4_Features.tex}

% Describe your model and implementation here. Refer to item 4. This may take around a page.
\section{Implementation}

Our model is a feedforward classification model. It is composed of TODO Conv2D layers of size TODO with ReLu activation, followed by a Dense layer with TODO units. The model has TODO parameters in total, taking up about TODO MB in total.

Before training, we optimized various parameters empirically. We set out certain augmentations and resolutions to be tested, then trained a model on every dimension of this space via nested for loops that can be found in our code. This process is further outlined in the Features section. The size of layers and the number of layers were determined by manual empirical testing.

% How are you evaluating your model? What results do you have so far? What are your baselines? Refer to item 5. This may take around 0.5 pages.
\section{Results and Evaluation}
\input{6_Results_and_Evaluation.tex}

% Write about your plans for the remainder of the project. This should include a discussion of the feedback you received from your TA, and how you plan to improve your approach. Reflect on your implementation and areas for improvement. Refer to item 6. This may be around 0.5 pages.
\section{Feedback and Plans}
The primary feedback focused on improving the reproducibility and interpretability of the results. 
Specifically, the TA recommended implementing a more consistent validation split across all preprocessing configurations to ensure that performance differences are not influenced by random sampling. 
Additionally, they advised that the baseline model should be clearly defined and quantitatively compared against augmented configurations to emphasize the measurable impact of each experimental change.
Another key piece of feedback was to include computational metrics—such as average training time per epoch and resource utilization—in the results table. 
This would clearly demonstrate trade-offs between model accuracy and computational efficiency, which is key to optimizing performance under limited resources.
The TA also suggested monitoring for potential overfitting by tracking training and validation accuracy curves more closely and introducing early stopping or dropout adjustments if the validation loss diverges.

For the remainder of the project, we plan to incorporate these recommendations by (1) locking a fixed random seed for reproducibility, (2) ensuring the dataset splits are stratified, (3) expanding the evaluation metrics to include F1-score and confusion matrices for a more detailed performance assessment, and (4) introducing a systematic summary table comparing all tested resolutions and augmentation levels.
We will also document the data preprocessing pipeline in greater detail to improve transparency and ensure that results are easily replicable. 
Finally, once the best-performing configuration is identified, we will retrain the model for additional epochs and evaluate it on a held-out test set to provide final quantitative results and visualizations.





% *****************************************************************************************
% OPTIONAL SECTION 8: TEMPLATE NOTES, REFERENCES, TABLES AND FIGURES, EQUATIONS
% \input{8_Template_Notes}
% \section*{Limitations}
% *****************************************************************************************


% Write in this section a few sentences describing the contributions of each team member. What did each member work on? Refer to item 7.
\section*{Team Contributions}
\input{9_Team Contributions.tex}


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{custom,anthology-overleaf-1,anthology-overleaf-2}
% Custom bibliography entries only
\bibliographystyle{acl_natbib}
\bibliography{custom}

% \appendix
% \section{Example Appendix}
% \label{sec:appendix}
% This is an appendix.

\end{document}
