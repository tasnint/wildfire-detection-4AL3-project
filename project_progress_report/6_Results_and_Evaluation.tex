Our model evaluation process focuses on comparing different image preprocessing and augmentation configurations to identify the combination that offers the best trade-off between computational efficiency and classification accuracy.
The array of pixel sizes we are testing includes pixel sizes, rotation, shifting height and width, zoom, flip, and brightness.
Each configuration was trained for five epochs, and both validation accuracy and average computation time per epoch were recorded to measure performance.
The system automatically stops iterating over new resolutions when accuracy improvements fall below 3\%, reducing unnecessary computation.

Initial experiments at lower resolutions (128x128) with standard augmentation (rotation, brightness, and zoom variations) achieved validation accuracies around 57 - 58\%.

Larger image resolutions are currently being evaluated to determine whether higher spatial detail leads to significant accuracy gains. The best configuration identified so far will be used to train the final CNN model, which is then validated and tested on separate data subsets.

The final architecture employs a four-block convolutional neural network (Conv2D-MaxPooling layers) with dropout for regularization, compiled with binary cross-entropy loss, a metric of accuracy, and the Adam optimizer. Model performance is monitored across epochs using accuracy and loss curves saved as visual outputs. Test performance will be reported once all preprocessing configurations finish executing. For baselines, the model without augmentation and at lower resolutions serves as the control, while subsequent tests compare the impact of increasing image size and data augmentation strength.

Our current best model uses no augmentation and normalizes images to 224x224. This model achieves a training accuracy of 83.73\% and a validation accuracy of 79.85\%. However, this model is an early experiment and many areas of improvement were raised during the experimentation process. as outlined in the Feedback and Plans section.

% To evaluate model performance, the following classification metrics are computed on the test set:
% \setlength{\tabcolsep}{2pt}
%
% >>>>>>> origin/tanisha-2
% \begin{table}[h!]
% \centering
% \begin{tabular}{lcccc}
% \hline
% \textbf{Metric} & \textbf{Fire} & \textbf{No Fire} & \textbf{Weighted Avg.} & \textbf{Support} \\
% \hline
% Precision & xxx & xxx & xxx & xxx \\
% Recall    & xxx & xxx & xxx & xxx \\
% F1-Score  & xxx & xxx & xxx & xxx \\
% Accuracy  & \multicolumn{4}{c}{xxx} \\
% \hline
% \end{tabular}
% \caption{Model performance metrics on the test set (placeholders to be updated after full training).}
% \label{tab:performance_metrics}
% \end{table}
%
